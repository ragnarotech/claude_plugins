"""
BDD step definitions for RAG pipeline testing with DeepEval.

These steps wrap DeepEval metrics for RAG-specific evaluation:
- FaithfulnessMetric: Ensures responses are grounded in retrieval context
- AnswerRelevancyMetric: Measures relevance of response to query
- ContextualRelevancyMetric: Evaluates quality of retrieved context
- HallucinationMetric: Detects factual claims not supported by context
"""
import pytest
from pytest_bdd import given, when, then, parsers, scenarios
from deepeval.test_case import LLMTestCase
from deepeval.metrics import (
    FaithfulnessMetric,
    AnswerRelevancyMetric,
    HallucinationMetric,
)

# Load all scenarios from the RAG quality feature file
scenarios('../features/rag_quality.feature')


# ============================================================================
# GIVEN Steps - Test Setup
# ============================================================================

@given(parsers.parse('the LLM evaluator uses model "{model}"'))
def set_model(test_context, model):
    """Set the LLM model to use for evaluation."""
    test_context["model"] = model


@given(parsers.parse('the default threshold is {threshold:f}'))
def set_threshold(test_context, threshold):
    """Set the default threshold for metric evaluation."""
    test_context["threshold"] = threshold


@given(parsers.parse('a user query "{query}"'))
def set_query(test_context, query):
    """Set the user's input query."""
    test_context["input"] = query


@given(parsers.parse('the retrieval context contains:\n{doc_string}'))
def set_retrieval_context(test_context, doc_string):
    """
    Set the retrieval context (documents retrieved by RAG pipeline).

    The context is stored as a list with the document string.
    In production, this might be multiple documents.
    """
    test_context["retrieval_context"] = [doc_string.strip()]


@given(parsers.parse('a potentially adversarial prompt "{prompt}"'))
def set_adversarial_prompt(test_context, prompt):
    """Set an adversarial or edge-case prompt for safety testing."""
    test_context["input"] = prompt


@given(parsers.parse('the expected intent is "{intent}"'))
def set_expected_intent(test_context, intent):
    """Set the expected intent for classification tests."""
    test_context["expected_intent"] = intent


# ============================================================================
# WHEN Steps - Actions
# ============================================================================

@when(parsers.parse('the RAG pipeline generates response "{response}"'))
def set_rag_response(test_context, response):
    """
    Set the response generated by the RAG pipeline.

    Creates a DeepEval LLMTestCase with input, output, and retrieval context.
    """
    test_context["actual_output"] = response
    test_context["test_case"] = LLMTestCase(
        input=test_context["input"],
        actual_output=response,
        retrieval_context=test_context.get("retrieval_context", [])
    )


@when(parsers.parse('the chatbot generates a safe response "{response}"'))
def set_safe_response(test_context, response):
    """Set a safe response for safety validation tests."""
    test_context["actual_output"] = response
    test_context["test_case"] = LLMTestCase(
        input=test_context["input"],
        actual_output=response
    )


@when('the response is evaluated for semantic similarity')
def evaluate_semantic_similarity(test_context, mock_llm_client):
    """
    Evaluate semantic similarity between query and expected intent.

    In a real scenario, this would generate a response and check similarity.
    For demo purposes, we use a mock response.
    """
    # Generate mock response based on query
    response = mock_llm_client.generate(test_context["input"])
    test_context["actual_output"] = response
    test_context["test_case"] = LLMTestCase(
        input=test_context["input"],
        actual_output=response,
        expected_output=test_context.get("expected_intent", "")
    )


# ============================================================================
# THEN Steps - Assertions with DeepEval Metrics
# ============================================================================

@then(parsers.parse('the faithfulness score should be at least {min_score:f}'))
def check_faithfulness(test_context, min_score, record_property):
    """
    Assert that the response faithfulness meets minimum threshold.

    Faithfulness measures how well the response is grounded in the
    retrieval context without adding unsupported claims.
    """
    metric = FaithfulnessMetric(
        threshold=min_score,
        model=test_context["model"],
        include_reason=True
    )

    # Measure the metric
    metric.measure(test_context["test_case"])

    # Record results for reporting
    record_property("faithfulness_score", metric.score)
    record_property("faithfulness_reason", metric.reason)
    test_context["metrics"]["faithfulness"] = metric.score

    # Assert threshold
    assert metric.score >= min_score, \
        f"Faithfulness {metric.score:.2f} < {min_score}: {metric.reason}"


@then(parsers.parse('the answer relevancy score should be at least {min_score:f}'))
def check_relevancy(test_context, min_score, record_property):
    """
    Assert that answer relevancy meets minimum threshold.

    Relevancy measures how well the response addresses the user's query.
    """
    metric = AnswerRelevancyMetric(
        threshold=min_score,
        model=test_context["model"],
        include_reason=True
    )

    metric.measure(test_context["test_case"])

    # Record results
    record_property("relevancy_score", metric.score)
    record_property("relevancy_reason", metric.reason)
    test_context["metrics"]["relevancy"] = metric.score

    # Assert threshold
    assert metric.score >= min_score, \
        f"Relevancy {metric.score:.2f} < {min_score}: {metric.reason}"


@then('the response should fail faithfulness check')
def check_faithfulness_failure(test_context, record_property):
    """
    Assert that the response FAILS faithfulness (detects hallucination).

    This is used for negative test cases where we expect the model to
    generate content not supported by the context.
    """
    metric = FaithfulnessMetric(
        threshold=0.7,  # Reasonable threshold
        model=test_context["model"],
        include_reason=True
    )

    metric.measure(test_context["test_case"])

    record_property("faithfulness_score", metric.score)
    record_property("faithfulness_reason", metric.reason)

    # For hallucination detection, we expect LOW faithfulness scores
    assert metric.score < 0.7, \
        f"Expected hallucination but got faithfulness {metric.score:.2f}: {metric.reason}"


@then(parsers.parse('the toxicity score should be below {max_score:f}'))
def check_toxicity(test_context, max_score, record_property):
    """
    Assert that toxicity is below maximum threshold.

    Note: DeepEval's ToxicityMetric returns higher scores for more toxic content.
    We want LOW scores for safe responses.
    """
    # Import here to avoid issues if metric not available
    try:
        from deepeval.metrics import ToxicityMetric

        metric = ToxicityMetric(
            threshold=max_score,
            model=test_context["model"]
        )

        metric.measure(test_context["test_case"])

        record_property("toxicity_score", metric.score)
        test_context["metrics"]["toxicity"] = metric.score

        assert metric.score <= max_score, \
            f"Toxicity {metric.score:.2f} > {max_score}"

    except ImportError:
        pytest.skip("ToxicityMetric not available in this DeepEval version")


@then('the response should be appropriate')
def check_appropriateness(test_context):
    """
    General check that response is appropriate (not adversarial).

    This is a placeholder for custom appropriateness logic.
    In production, you might use GEval with custom criteria.
    """
    response = test_context.get("actual_output", "")

    # Simple check: response should not contain obvious refusal patterns
    # AND should not contain adversarial content
    assert len(response) > 0, "Response should not be empty"
    assert "system prompt" not in response.lower(), \
        "Response should not leak system information"


@then(parsers.parse('the relevancy score should meet threshold {threshold:f}'))
def check_relevancy_threshold(test_context, threshold, record_property):
    """
    Check that relevancy meets a specific threshold.

    Similar to check_relevancy but uses threshold from scenario.
    """
    metric = AnswerRelevancyMetric(
        threshold=threshold,
        model=test_context["model"],
        include_reason=True
    )

    metric.measure(test_context["test_case"])

    record_property("relevancy_score", metric.score)
    test_context["metrics"]["relevancy"] = metric.score

    assert metric.score >= threshold, \
        f"Relevancy {metric.score:.2f} < {threshold}: {metric.reason}"
