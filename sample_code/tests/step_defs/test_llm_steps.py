"""
BDD step definitions for general LLM evaluation with DeepEval.

These steps cover:
- Answer quality evaluation
- Bias and toxicity detection
- Custom G-Eval criteria
- Cross-domain testing
"""
import pytest
from pytest_bdd import given, when, then, parsers, scenarios
from deepeval.test_case import LLMTestCase
from deepeval.metrics import (
    AnswerRelevancyMetric,
    BiasMetric,
    GEval,
)

# Load all scenarios from the LLM evaluation feature file
scenarios('../features/llm_evaluation.feature')


# ============================================================================
# GIVEN Steps - Test Setup
# ============================================================================

@given(parsers.parse('the expected output "{expected}"'))
def set_expected_output(test_context, expected):
    """Set the expected output for comparison."""
    test_context["expected_output"] = expected


@given(parsers.parse('custom evaluation criteria:\n{criteria}'))
def set_custom_criteria(test_context, criteria):
    """
    Set custom evaluation criteria for G-Eval.

    The criteria is stored as a string and will be parsed
    into evaluation steps for GEval metric.
    """
    # Parse criteria into a list of steps
    criteria_lines = [
        line.strip().lstrip('-').strip()
        for line in criteria.split('\n')
        if line.strip() and line.strip().startswith('-')
    ]
    test_context["custom_criteria"] = criteria_lines


@given(parsers.parse('a user query in domain "{domain}"'))
def set_domain(test_context, domain):
    """Set the domain context for the query."""
    test_context["domain"] = domain


@given(parsers.parse('the query is "{query}"'))
def set_query_for_domain(test_context, query):
    """Set the query for domain-specific testing."""
    test_context["input"] = query


# ============================================================================
# WHEN Steps - Actions
# ============================================================================

@when(parsers.parse('the LLM generates response "{response}"'))
def set_llm_response(test_context, response):
    """
    Set the response generated by the LLM.

    Creates a DeepEval LLMTestCase with input and output.
    """
    test_context["actual_output"] = response
    test_context["test_case"] = LLMTestCase(
        input=test_context["input"],
        actual_output=response,
        expected_output=test_context.get("expected_output")
    )


@when('the LLM generates a response')
def generate_llm_response(test_context, mock_llm_client):
    """
    Generate a response using the LLM client.

    In production, this would call the actual LLM API.
    For testing, we use a mock client.
    """
    response = mock_llm_client.generate(test_context["input"])
    test_context["actual_output"] = response
    test_context["test_case"] = LLMTestCase(
        input=test_context["input"],
        actual_output=response,
        expected_output=test_context.get("expected_output")
    )


# ============================================================================
# THEN Steps - Assertions with DeepEval Metrics
# ============================================================================

@then(parsers.parse('the answer relevancy score should be at least {min_score:f}'))
def check_answer_relevancy(test_context, min_score, record_property):
    """
    Assert that answer relevancy meets minimum threshold.

    This is a duplicate of the step in test_rag_steps.py, but pytest-bdd
    allows step reuse across different feature files.
    """
    metric = AnswerRelevancyMetric(
        threshold=min_score,
        model=test_context["model"],
        include_reason=True
    )

    metric.measure(test_context["test_case"])

    record_property("relevancy_score", metric.score)
    record_property("relevancy_reason", metric.reason)
    test_context["metrics"]["relevancy"] = metric.score

    assert metric.score >= min_score, \
        f"Relevancy {metric.score:.2f} < {min_score}: {metric.reason}"


@then('the response should be considered successful')
def check_response_success(test_context):
    """
    General success check for the response.

    Validates that the response is non-empty and meets basic quality criteria.
    """
    response = test_context.get("actual_output", "")

    assert len(response) > 0, "Response should not be empty"
    assert len(response.split()) >= 5, \
        "Response should contain at least 5 words for meaningful content"


@then(parsers.parse('the bias score should be below {max_score:f}'))
def check_bias(test_context, max_score, record_property):
    """
    Assert that bias score is below maximum threshold.

    BiasMetric detects potential biases related to gender, race, etc.
    Lower scores indicate less bias.
    """
    try:
        metric = BiasMetric(
            threshold=max_score,
            model=test_context["model"]
        )

        metric.measure(test_context["test_case"])

        record_property("bias_score", metric.score)
        test_context["metrics"]["bias"] = metric.score

        assert metric.score <= max_score, \
            f"Bias {metric.score:.2f} > {max_score}"

    except ImportError:
        pytest.skip("BiasMetric not available in this DeepEval version")


@then(parsers.parse('the toxicity score should be below {max_score:f}'))
def check_toxicity_llm(test_context, max_score, record_property):
    """
    Assert that toxicity is below maximum threshold.

    Duplicate of RAG step for use in LLM evaluation scenarios.
    """
    try:
        from deepeval.metrics import ToxicityMetric

        metric = ToxicityMetric(
            threshold=max_score,
            model=test_context["model"]
        )

        metric.measure(test_context["test_case"])

        record_property("toxicity_score", metric.score)
        test_context["metrics"]["toxicity"] = metric.score

        assert metric.score <= max_score, \
            f"Toxicity {metric.score:.2f} > {max_score}"

    except ImportError:
        pytest.skip("ToxicityMetric not available in this DeepEval version")


@then('the bias score should indicate potential bias')
def check_bias_detected(test_context, record_property):
    """
    Assert that bias IS detected (negative test case).

    This is used for scenarios where we expect the model to generate
    biased content, to validate our bias detection works.
    """
    try:
        metric = BiasMetric(
            threshold=0.5,  # Reasonable threshold
            model=test_context["model"]
        )

        metric.measure(test_context["test_case"])

        record_property("bias_score", metric.score)

        # For bias detection, we expect HIGHER scores
        assert metric.score > 0.3, \
            f"Expected bias but got low score {metric.score:.2f}"

    except ImportError:
        pytest.skip("BiasMetric not available in this DeepEval version")


@then(parsers.parse('the custom criteria score should be at least {min_score:f}'))
def check_custom_criteria(test_context, min_score, record_property):
    """
    Evaluate response using custom G-Eval criteria.

    GEval allows defining custom evaluation criteria in natural language,
    making it perfect for BDD where requirements are expressed in business terms.
    """
    criteria_steps = test_context.get("custom_criteria", [])

    if not criteria_steps:
        pytest.fail("No custom criteria defined. Use 'And custom evaluation criteria:' step.")

    # Create GEval metric with custom criteria
    metric = GEval(
        name="Custom Criteria",
        criteria="Evaluate the response based on the following requirements",
        evaluation_steps=criteria_steps,
        threshold=min_score,
        model=test_context["model"]
    )

    metric.measure(test_context["test_case"])

    record_property("custom_criteria_score", metric.score)
    record_property("custom_criteria_reason", metric.reason)
    test_context["metrics"]["custom_criteria"] = metric.score

    assert metric.score >= min_score, \
        f"Custom criteria score {metric.score:.2f} < {min_score}: {metric.reason}"


@then(parsers.parse('the answer relevancy score should be at least {min_relevancy:f}'))
def check_domain_relevancy(test_context, min_relevancy, record_property):
    """
    Check relevancy for domain-specific scenarios.

    This is used in scenario outlines with different thresholds per domain.
    """
    metric = AnswerRelevancyMetric(
        threshold=min_relevancy,
        model=test_context["model"],
        include_reason=True
    )

    metric.measure(test_context["test_case"])

    domain = test_context.get("domain", "unknown")
    record_property(f"{domain}_relevancy_score", metric.score)
    test_context["metrics"]["relevancy"] = metric.score

    assert metric.score >= min_relevancy, \
        f"Domain '{domain}' relevancy {metric.score:.2f} < {min_relevancy}: {metric.reason}"
